# Estudo de Caso Dimensional

üìå 
Este projeto √© um estudo de caso sobre modelagem dimensional utilizando **Apache Spark**. Ele realiza a extra√ß√£o, transforma√ß√£o e carga (ETL) de um dataset armazenado no formato **Parquet**, criando tabelas dimensionais a partir de um conjunto de dados estruturado.

üöÄ Tecnologias Utilizadas
- **Python** - Linguagem principal do projeto
- **Apache Spark** - Processamento distribu√≠do de dados
- **PySpark** - API do Apache Spark para Python
- **Google Colab** - Ambiente de execu√ß√£o para rodar o projeto online
- **Parquet** - Formato de armazenamento eficiente para Big Data

‚úÖ Estrutura do Projeto
- **dimensional_dataset.zip**: Arquivo compactado contendo os dados
- **Estudo_de_caso_Projeto_real.ipynb**: Notebook principal com c√≥digo ETL

üîó Como Clonar e Executar no Google Colab
1. Clone o reposit√≥rio:
   ```bash
   git clone https://github.com/Emanuel-nx/Estudo_de_Caso_Dimensional.git
   ```
2. Acesse [Google Colab](https://colab.research.google.com/)
3. Envie o arquivo `Estudo_de_caso_Projeto_real.ipynb` para o Colab
4. Execute as c√©lulas do notebook

üìë Explica√ß√£o do C√≥digo
### Instala√ß√£o de Depend√™ncias
```python
!pip install pyspark
```
Instala a biblioteca **PySpark** caso ainda n√£o esteja instalada.

### Extra√ß√£o dos Dados
```python
!unzip dimensional_dataset.zip -d dimensional_dataset
```
Descompacta o dataset para uma pasta chamada `dimensional_dataset`.

### Inicializa√ß√£o do Spark
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("dimensional").getOrCreate()
```
Cria uma **SparkSession**, necess√°ria para utilizar as funcionalidades do Apache Spark.

### Leitura do Dataset
```python
df = spark.read.parquet('dimensional_dataset/content/dimensional_dataset')
df.show(5)
```
Carrega os dados do formato **Parquet** e exibe as primeiras linhas.

### Cria√ß√£o da Dimens√£o `People`
```python
people_cols = ['person_id', 'person_name', 'birth_date', 'email', 'phone_number', 'job']
people_df = df.select(*people_cols).distinct()
people_df.show(5)
```
Seleciona e remove duplicatas das colunas relacionadas a pessoas.

### Cria√ß√£o da Dimens√£o `Cities`
```python
from pyspark.sql import functions as F
cities_cols = ['city_name', 'zip_code', F.col('country_name').alias('country')]
cities_df = df.select(*cities_cols).distinct()
cities_df.show(5)
```
Seleciona e renomeia colunas para criar a dimens√£o de cidades.

## Contribui√ß√£o
Sinta-se √† vontade para criar **issues** ou **pull requests** no reposit√≥rio!

## Autor
[Emanuel-nx](https://github.com/Emanuel-nx)

